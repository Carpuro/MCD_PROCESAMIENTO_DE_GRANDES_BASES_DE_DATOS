{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "so33orgKTUNr"
   },
   "source": [
    "**Creación de un RDD desde una lista**\n",
    "\n",
    "En este ejemplo, creamos un RDD a partir de una lista de números y luego aplicamos una transformación para multiplicar cada número por 2. La acción collect() recopila los resultados y los muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F2m06S0BTUNs"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWY_92oATUNt",
    "outputId": "2db540eb-5b5e-43c0-e7e4-4cc95522df3b"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=EjemploRDD, master=local) created by __init__ at C:\\Users\\carlo\\AppData\\Local\\Temp\\ipykernel_188432\\2633541255.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Crea un contexto Spark\u001b[39;00m\n\u001b[32m      3\u001b[39m local = \u001b[33m\"\u001b[39m\u001b[33mlocal[*]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEjemploRDD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#sc=  SparkContext(appName=\"MyAppName\")\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Crea un RDD a partir de una lista\u001b[39;00m\n\u001b[32m      8\u001b[39m rdd = sc.parallelize([\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\python\\pyspark\\context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\spark\\python\\pyspark\\context.py:449\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    446\u001b[39m     callsite = SparkContext._active_spark_context._callsite\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot run multiple SparkContexts at once; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m         % (\n\u001b[32m    454\u001b[39m             currentAppName,\n\u001b[32m    455\u001b[39m             currentMaster,\n\u001b[32m    456\u001b[39m             callsite.function,\n\u001b[32m    457\u001b[39m             callsite.file,\n\u001b[32m    458\u001b[39m             callsite.linenum,\n\u001b[32m    459\u001b[39m         )\n\u001b[32m    460\u001b[39m     )\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     SparkContext._active_spark_context = instance\n",
      "\u001b[31mValueError\u001b[39m: Cannot run multiple SparkContexts at once; existing SparkContext(app=EjemploRDD, master=local) created by __init__ at C:\\Users\\carlo\\AppData\\Local\\Temp\\ipykernel_188432\\2633541255.py:4 "
     ]
    }
   ],
   "source": [
    "# Crea un contexto Spark\n",
    "sc = SparkContext(\"local\", \"EjemploRDD\")\n",
    "#sc=  SparkContext(appName=\"MyAppName\")\n",
    "\n",
    "# Crea un RDD a partir de una lista\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Realiza una transformación: multiplica cada elemento por 2\n",
    "rdd_multiplicado = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Realiza una acción: muestra los resultados\n",
    "print(rdd_multiplicado.collect())\n",
    "#rdd_multiplicado.collect()\n",
    "# Detén el contexto Spark\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wexD7pcTUNt"
   },
   "source": [
    "La línea que proporcionaste es un fragmento de código PySpark que inicializa un nuevo SparkContext. Desglosaré cada parte de la línea:\n",
    "\n",
    "\n",
    "**sc = SparkContext(\"local\", \"EjemploRDD\")**\n",
    "\n",
    "* **sc:** Es la variable donde estás guardando tu nuevo SparkContext.\n",
    "* **SparkContext:** Es una clase de PySpark que representa la conexión a un clúster de Spark y se utiliza para crear RDDs, acumuladores y variables de transmisión en ese clúster.\n",
    "* **\"local\":** Es el modo maestro que estás especificando para tu SparkContext. El string \"local\" indica que Spark debería ejecutarse en un modo local de un solo núcleo. Puedes especificar \"local[n]\" para usar n núcleos; por ejemplo, \"local[4]\" usaría 4 núcleos.\n",
    "* **\"EjemploRDD\":** Es el nombre que estás dando a tu aplicación Spark. Este nombre aparecerá en tu interfaz de usuario de Spark y en los registros para ayudarte a diferenciar entre diferentes aplicaciones Spark que podrías estar ejecutando.\n",
    "\n",
    "\n",
    "Resilient Distributed Dataset (RDD) a partir de una lista de Python. Vamos a desglosar cada parte de esta línea:\n",
    "\n",
    "\n",
    "**rdd = sc.parallelize([1, 2, 3, 4, 5])**\n",
    "\n",
    "* **rdd:** Es el nombre que le estás dando a tu nuevo RDD.\n",
    "\n",
    "* **sc:** Es el SparkContext que estás utilizando para crear el RDD. El **SparkContext** es una client-side object que representa la conexión a un clúster de Spark, y se utiliza para crear RDDs, acumuladores y variables de transmisión en ese clúster.\n",
    "\n",
    "* **parallelize:** Es un método de **SparkContext** que crea un nuevo RDD a partir de una colección iterable (en este caso, una lista de Python). El RDD resultante tiene elementos que están distribuidos en los nodos del clúster.\n",
    "\n",
    "* **[1, 2, 3, 4, 5]:** Es la lista de Python que estás usando como fuente de datos para tu RDD. Contiene cinco elementos: los números del 1 al 5\n",
    "\n",
    "**rdd_multiplicado = rdd.map(lambda x: x * 2)**\n",
    "\n",
    "* **rdd_multiplicado:** Es la variable donde estás guardando tu nuevo RDD, que será el resultado de aplicar una función a cada elemento del RDD original (rdd).\n",
    "\n",
    "* **rdd:** Es el RDD original sobre el que estás realizando la operación. Es el RDD que creaste en la línea anterior del código que analizamos anteriormente.\n",
    "\n",
    "* **map:** Es un método que toma una función como argumento y aplica esa función a cada elemento del RDD, produciendo un nuevo RDD.\n",
    "\n",
    "* **lambda x: x * 2:** Es la función que estás aplicando a cada elemento del RDD. Esta función toma un argumento (x) y devuelve el resultado de multiplicar x por 2.\n",
    "\n",
    "**print(rdd_multiplicado.collect())**\n",
    "\n",
    "* **rdd_multiplicado:** Es el RDD sobre el cual estás realizando operaciones. Este RDD fue creado en una línea anterior de tu código y contiene los resultados de multiplicar cada elemento de un RDD original por 2.\n",
    "\n",
    "* **collect():** Es un método que se utiliza para recuperar todos los elementos de un RDD a la memoria del programa driver como una lista de Python. Este es un tipo de acción en Spark, lo que significa que desencadena la ejecución de todas las transformaciones que se han definido anteriormente en el flujo de trabajo de Spark.\n",
    "\n",
    "* **print(...):** Es una función de Python que se utiliza para imprimir su argumento a la consola. En este caso, estás imprimiendo la lista de elementos que fue retornada por rdd_multiplicado.collect()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzkFTlshTUNu"
   },
   "source": [
    "**Carga de un RDD desde un archivo de texto**\n",
    "\n",
    "En este ejemplo, cargamos un archivo de texto como un RDD y luego aplicamos una transformación para filtrar las líneas que contienen la palabra \"Python\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsE_WUayTUNu",
    "outputId": "d5f66c5b-af70-4797-8ed9-e45c552c4df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python, conocido por su simplicidad y versatilidad, es uno de los lenguajes de programación más populares en el mundo de la informática. ', 'En el campo de la inteligencia artificial, Python se destaca con bibliotecas como TensorFlow y PyTorch, que impulsan avances en el aprendizaje profundo. ', 'Además, Python es una excelente opción para la automatización de tareas, desde la administración de servidores hasta la creación de scripts para simplificar procesos.', 'Con su amplio ecosistema de bibliotecas y su legado en la comunidad de código abierto, Python sigue siendo un lenguaje poderoso y en constante crecimiento ']\n"
     ]
    }
   ],
   "source": [
    "#from pyspark import SparkContext\n",
    "\n",
    "# Crea un contexto Spark\n",
    "sc = SparkContext(\"local\", \"EjemploRDD\")\n",
    "\n",
    "# Carga un archivo de texto como un RDD\n",
    "rdd = sc.textFile(\"archivo.txt\")\n",
    "\n",
    "# Realiza una transformación: filtra líneas que contienen la palabra \"Python\"\n",
    "rdd_filtrado = rdd.filter(lambda linea: \"Python\" in linea)\n",
    "\n",
    "# Realiza una acción: muestra los resultados\n",
    "print(rdd_filtrado.collect())\n",
    "\n",
    "# Detén el contexto Spark\n",
    "sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j625Cfn9TUNu"
   },
   "source": [
    "**Reducción de un RDD**\n",
    "\n",
    "En este ejemplo, creamos un RDD a partir de una lista de números y luego aplicamos una reducción para calcular la suma de todos los elementos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdnrhLR8TUNu",
    "outputId": "9f56330d-94f8-41db-da00-32ec3fffc17b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#from pyspark import SparkContext\n",
    "\n",
    "# Crea un contexto Spark\n",
    "sc = SparkContext(\"local\", \"EjemploRDD\")\n",
    "\n",
    "# Crea un RDD a partir de una lista de números\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Realiza una reducción para sumar todos los elementos\n",
    "suma = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Muestra el resultado\n",
    "print(suma)\n",
    "\n",
    "# Detén el contexto Spark\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
